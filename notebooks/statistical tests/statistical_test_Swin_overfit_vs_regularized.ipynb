{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22fc1f4e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-15T20:03:07.467710Z",
     "iopub.status.busy": "2025-09-15T20:03:07.467525Z",
     "iopub.status.idle": "2025-09-15T20:04:57.051081Z",
     "shell.execute_reply": "2025-09-15T20:04:57.050231Z"
    },
    "papermill": {
     "duration": 109.587979,
     "end_time": "2025-09-15T20:04:57.052256",
     "exception": false,
     "start_time": "2025-09-15T20:03:07.464277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE CALIBRATION ANALYSIS - PATCH AGGREGATION\n",
      "============================================================\n",
      "\n",
      "1. COLLECTING PATCH-LEVEL DATA...\n",
      "Found 227 images to process...\n",
      "Processing image 1/227\n",
      "Processing image 51/227\n",
      "Processing image 101/227\n",
      "Processing image 151/227\n",
      "Processing image 201/227\n",
      "Successfully processed 227 images with 3140 total patches\n",
      "\n",
      "2. AGGREGATING PER PAINTING WITH MAJORITY VOTING...\n",
      "\n",
      "3. BASIC PERFORMANCE:\n",
      "   Number of paintings: 227\n",
      "   Total patches processed: 3140\n",
      "   Average patches per painting: 13.8\n",
      "   Overfit accuracy: 0.9692\n",
      "   Regularized accuracy: 0.9692\n",
      "   True labels: 109 authentic, 118 imitation\n",
      "   Overfit confidence range: [0.500, 1.000]\n",
      "   Regularized confidence range: [0.500, 1.000]\n",
      "\n",
      "4. CALIBRATION METRICS:\n",
      "\n",
      "   Expected Calibration Error (ECE):\n",
      "     Overfit ECE: 0.0179\n",
      "     Regularized ECE: 0.0165\n",
      "     Difference (reg - overfit): -0.0014\n",
      "     P(Regularized better): 0.5790\n",
      "     95% CI of difference: [-0.0209, 0.0174]\n",
      "     Result: Regularized model is better calibrated\n",
      "\n",
      "   Brier Score:\n",
      "     Overfit Brier: 0.0256\n",
      "     Regularized Brier: 0.0256\n",
      "     Difference (reg - overfit): 0.0000\n",
      "     P(Regularized better): 0.4890\n",
      "     95% CI of difference: [-0.0091, 0.0085]\n",
      "     Result: Overfit model has better Brier score\n",
      "\n",
      "   Log Loss:\n",
      "     Overfit Log Loss: 0.0842\n",
      "     Regularized Log Loss: 0.0831\n",
      "     Difference (reg - overfit): -0.0012\n",
      "     P(Regularized better): 0.5205\n",
      "     95% CI of difference: [-0.0288, 0.0230]\n",
      "     Result: Regularized model has better log loss\n",
      "\n",
      "5. PATCH CONSISTENCY ANALYSIS:\n",
      "   Overfit mean patch variance: 0.0237\n",
      "   Regularized mean patch variance: 0.0176\n",
      "   Difference (reg - overfit): -0.0061\n",
      "   P(Regularized more consistent): 1.0000\n",
      "   Result: Regularized model is more consistent\n",
      "\n",
      "6. OVERALL SUPERIORITY ASSESSMENT:\n",
      "   Evidence for regularized model: 3 out of 4 metrics\n",
      "   - Better calibration (ECE)\n",
      "   - Better log loss\n",
      "   - Better patch consistency\n",
      "\n",
      "   CONCLUSION: Regularized model is CLEARLY SUPERIOR\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import stats\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class CalibrationTester:\n",
    "    def __init__(self, overfit_model_path, regularized_model_path, device=None):\n",
    "        if device is None:\n",
    "            if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "                self.device = torch.device(\"mps\")\n",
    "            elif torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(device)\n",
    "            \n",
    "        self.class_to_idx = {'authentic': 0, 'imitation': 1}\n",
    "        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
    "        \n",
    "        # Load models\n",
    "        self.overfit_model = self._load_swin_model(overfit_model_path, regularized=False)\n",
    "        self.regularized_model = self._load_swin_model(regularized_model_path, regularized=True)\n",
    "        \n",
    "        # Transform\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def _load_swin_model(self, model_path, regularized=False):\n",
    "        if regularized:\n",
    "            model = timm.create_model(\n",
    "                'swin_tiny_patch4_window7_224',\n",
    "                pretrained=False,\n",
    "                num_classes=2,\n",
    "                img_size=256,\n",
    "                drop_rate=0.5,\n",
    "                drop_path_rate=0.4\n",
    "            )\n",
    "        else:\n",
    "            model = timm.create_model(\n",
    "                'swin_tiny_patch4_window7_224',\n",
    "                pretrained=False,\n",
    "                num_classes=2,\n",
    "                img_size=256\n",
    "            )\n",
    "        \n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def _extract_patches(self, image):\n",
    "        w, h = image.size\n",
    "        max_dim = max(w, h)\n",
    "        \n",
    "        if max_dim > 1024:\n",
    "            grid_size = 4\n",
    "        elif max_dim >= 512:\n",
    "            grid_size = 2\n",
    "        else:\n",
    "            grid_size = 1\n",
    "            \n",
    "        patches = []\n",
    "        \n",
    "        if grid_size == 1:\n",
    "            min_dim = min(w, h)\n",
    "            if min_dim < 256:\n",
    "                patches.append(image.resize((256, 256)))\n",
    "            else:\n",
    "                left = (w - min_dim) // 2\n",
    "                top = (h - min_dim) // 2\n",
    "                patches.append(image.crop((left, top, left + min_dim, top + min_dim)))\n",
    "        else:\n",
    "            patch_width = w // grid_size\n",
    "            patch_height = h // grid_size\n",
    "            \n",
    "            for i in range(grid_size):\n",
    "                for j in range(grid_size):\n",
    "                    left = j * patch_width\n",
    "                    upper = i * patch_height\n",
    "                    right = (j + 1) * patch_width if (j + 1) < grid_size else w\n",
    "                    bottom = (i + 1) * patch_height if (i + 1) < grid_size else h\n",
    "                    \n",
    "                    patch = image.crop((left, upper, right, bottom))\n",
    "                    if patch.size[0] > 0 and patch.size[1] > 0:\n",
    "                        patches.append(patch)\n",
    "        \n",
    "        return patches\n",
    "\n",
    "    def collect_patch_level_data(self, test_folder_path, excel_path, sheet_name='vg_cv_data_july31'):\n",
    "        \"\"\"Collect patch-level probabilities following the pattern from your code\"\"\"\n",
    "        folder_path = Path(test_folder_path)\n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
    "        \n",
    "        image_files = []\n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(folder_path.glob(f'*{ext}'))\n",
    "            image_files.extend(folder_path.glob(f'*{ext.upper()}'))\n",
    "        \n",
    "        # Get true labels from Excel\n",
    "        excel_df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "        \n",
    "        print(f\"Found {len(image_files)} images to process...\")\n",
    "        \n",
    "        # Collections for patch-level data\n",
    "        painting_ids = []\n",
    "        true_labels = []\n",
    "        overfit_probs = []\n",
    "        regularized_probs = []\n",
    "        \n",
    "        valid_images = 0\n",
    "        \n",
    "        for i, image_path in enumerate(image_files):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Processing image {i+1}/{len(image_files)}\")\n",
    "            \n",
    "            # Get true label from Excel\n",
    "            excel_row = excel_df[excel_df['image'] == image_path.name]\n",
    "            if excel_row.empty:\n",
    "                continue\n",
    "                \n",
    "            is_authentic = excel_row.iloc[0]['is_wikiart_vangogh_oil_painting']\n",
    "            true_label = 0 if is_authentic == 1 else 1  # 0=authentic, 1=imitation\n",
    "            \n",
    "            try:\n",
    "                # Extract patches\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                patches = self._extract_patches(image)\n",
    "                \n",
    "                # Process each patch\n",
    "                for patch_idx, patch in enumerate(patches):\n",
    "                    patch_tensor = self.transform(patch).unsqueeze(0).to(self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # Overfit model\n",
    "                        overfit_logits = self.overfit_model(patch_tensor)\n",
    "                        overfit_prob = F.softmax(overfit_logits, dim=1)[0, 1].cpu().numpy()  # P(imitation)\n",
    "                        \n",
    "                        # Regularized model\n",
    "                        reg_logits = self.regularized_model(patch_tensor)\n",
    "                        reg_prob = F.softmax(reg_logits, dim=1)[0, 1].cpu().numpy()  # P(imitation)\n",
    "                    \n",
    "                    # Store patch-level data (using image name as painting ID)\n",
    "                    painting_ids.append(image_path.stem)  # Use filename without extension as ID\n",
    "                    true_labels.append(true_label)\n",
    "                    overfit_probs.append(float(overfit_prob))\n",
    "                    regularized_probs.append(float(reg_prob))\n",
    "                \n",
    "                valid_images += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path.name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully processed {valid_images} images with {len(painting_ids)} total patches\")\n",
    "        \n",
    "        return painting_ids, true_labels, overfit_probs, regularized_probs\n",
    "\n",
    "    def aggregate_per_painting(self, painting_ids, true_labels, overfit_probs, regularized_probs):\n",
    "        \"\"\"Aggregate patch-level predictions per painting using majority voting\"\"\"\n",
    "        \n",
    "        # Group patches by painting\n",
    "        paintings = defaultdict(list)\n",
    "        for pid, y, p1, p2 in zip(painting_ids, true_labels, overfit_probs, regularized_probs):\n",
    "            paintings[pid].append((y, p1, p2))\n",
    "        \n",
    "        # Aggregate results\n",
    "        painting_true = []\n",
    "        painting_overfit_major = []\n",
    "        painting_reg_major = []\n",
    "        painting_overfit_probs = []\n",
    "        painting_reg_probs = []\n",
    "        painting_overfit_variances = []\n",
    "        painting_reg_variances = []\n",
    "        painting_overfit_confidences = []\n",
    "        painting_reg_confidences = []\n",
    "        \n",
    "        for pid, patches in paintings.items():\n",
    "            y_true = patches[0][0]  # All patches have same true label\n",
    "            overfit_patch_probs = [t[1] for t in patches]\n",
    "            reg_patch_probs = [t[2] for t in patches]\n",
    "            \n",
    "            # Votes at 0.5 threshold\n",
    "            overfit_votes = [int(p >= 0.5) for p in overfit_patch_probs]\n",
    "            reg_votes = [int(p >= 0.5) for p in reg_patch_probs]\n",
    "            \n",
    "            # Majority voting\n",
    "            overfit_majority = Counter(overfit_votes).most_common(1)[0][0]\n",
    "            reg_majority = Counter(reg_votes).most_common(1)[0][0]\n",
    "            \n",
    "            # Calculate confidence as proportion of patches voting for majority\n",
    "            overfit_vote_counts = Counter(overfit_votes)\n",
    "            reg_vote_counts = Counter(reg_votes)\n",
    "            \n",
    "            overfit_confidence = overfit_vote_counts[overfit_majority] / len(overfit_votes)\n",
    "            reg_confidence = reg_vote_counts[reg_majority] / len(reg_votes)\n",
    "            \n",
    "            # Store results\n",
    "            painting_true.append(y_true)\n",
    "            painting_overfit_major.append(overfit_majority)\n",
    "            painting_reg_major.append(reg_majority)\n",
    "            painting_overfit_probs.append(overfit_patch_probs)\n",
    "            painting_reg_probs.append(reg_patch_probs)\n",
    "            painting_overfit_variances.append(np.var(overfit_patch_probs))\n",
    "            painting_reg_variances.append(np.var(reg_patch_probs))\n",
    "            painting_overfit_confidences.append(overfit_confidence)\n",
    "            painting_reg_confidences.append(reg_confidence)\n",
    "        \n",
    "        return {\n",
    "            'true_labels': np.array(painting_true),\n",
    "            'overfit_predictions': np.array(painting_overfit_major),\n",
    "            'reg_predictions': np.array(painting_reg_major),\n",
    "            'overfit_patch_probs': painting_overfit_probs,\n",
    "            'reg_patch_probs': painting_reg_probs,\n",
    "            'overfit_variances': np.array(painting_overfit_variances),\n",
    "            'reg_variances': np.array(painting_reg_variances),\n",
    "            'overfit_confidences': np.array(painting_overfit_confidences),\n",
    "            'reg_confidences': np.array(painting_reg_confidences)\n",
    "        }\n",
    "\n",
    "    def calculate_ece_for_voting(self, confidences, correct_predictions, n_bins=10):\n",
    "        \"\"\"Expected Calibration Error for majority voting confidences\"\"\"\n",
    "        bin_boundaries = np.linspace(0.5, 1.0, n_bins + 1)  # Only bins from 0.5 to 1.0 for vote proportions\n",
    "        bin_lowers = bin_boundaries[:-1]\n",
    "        bin_uppers = bin_boundaries[1:]\n",
    "        \n",
    "        ece = 0\n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "            in_bin = (confidences >= bin_lower) & (confidences <= bin_upper)\n",
    "            prop_in_bin = in_bin.mean()\n",
    "            \n",
    "            if prop_in_bin > 0:\n",
    "                accuracy_in_bin = correct_predictions[in_bin].mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "        \n",
    "        return ece\n",
    "\n",
    "    def bootstrap_calibration_comparison(self, overfit_confidences, reg_confidences, \n",
    "                                       overfit_correct, reg_correct, metric='ece', n_bootstrap=1000):\n",
    "        \"\"\"Bootstrap test comparing calibration metrics\"\"\"\n",
    "        n_samples = len(overfit_correct)\n",
    "        \n",
    "        # Calculate observed metrics\n",
    "        if metric == 'ece':\n",
    "            overfit_metric = self.calculate_ece_for_voting(overfit_confidences, overfit_correct)\n",
    "            reg_metric = self.calculate_ece_for_voting(reg_confidences, reg_correct)\n",
    "        elif metric == 'brier':\n",
    "            overfit_metric = brier_score_loss(overfit_correct, overfit_confidences)\n",
    "            reg_metric = brier_score_loss(reg_correct, reg_confidences)\n",
    "        elif metric == 'log_loss':\n",
    "            overfit_clipped = np.clip(overfit_confidences, 1e-15, 1-1e-15)\n",
    "            reg_clipped = np.clip(reg_confidences, 1e-15, 1-1e-15)\n",
    "            overfit_metric = log_loss(overfit_correct, overfit_clipped)\n",
    "            reg_metric = log_loss(reg_correct, reg_clipped)\n",
    "        \n",
    "        observed_diff = reg_metric - overfit_metric  # Negative = regularized better\n",
    "        \n",
    "        # Bootstrap\n",
    "        np.random.seed(42)\n",
    "        bootstrap_diffs = []\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            boot_overfit_conf = overfit_confidences[indices]\n",
    "            boot_reg_conf = reg_confidences[indices]\n",
    "            boot_overfit_correct = overfit_correct[indices]\n",
    "            boot_reg_correct = reg_correct[indices]\n",
    "            \n",
    "            # Check if bootstrap sample has both classes (needed for log_loss)\n",
    "            if metric == 'log_loss':\n",
    "                if len(np.unique(boot_overfit_correct)) < 2 or len(np.unique(boot_reg_correct)) < 2:\n",
    "                    continue  # Skip this bootstrap sample\n",
    "            \n",
    "            try:\n",
    "                if metric == 'ece':\n",
    "                    boot_overfit_metric = self.calculate_ece_for_voting(boot_overfit_conf, boot_overfit_correct)\n",
    "                    boot_reg_metric = self.calculate_ece_for_voting(boot_reg_conf, boot_reg_correct)\n",
    "                elif metric == 'brier':\n",
    "                    boot_overfit_metric = brier_score_loss(boot_overfit_correct, boot_overfit_conf)\n",
    "                    boot_reg_metric = brier_score_loss(boot_reg_correct, boot_reg_conf)\n",
    "                elif metric == 'log_loss':\n",
    "                    boot_overfit_clipped = np.clip(boot_overfit_conf, 1e-15, 1-1e-15)\n",
    "                    boot_reg_clipped = np.clip(boot_reg_conf, 1e-15, 1-1e-15)\n",
    "                    boot_overfit_metric = log_loss(boot_overfit_correct, boot_overfit_clipped)\n",
    "                    boot_reg_metric = log_loss(boot_reg_correct, boot_reg_clipped)\n",
    "                \n",
    "                bootstrap_diffs.append(boot_reg_metric - boot_overfit_metric)\n",
    "                \n",
    "            except ValueError:\n",
    "                # Skip bootstrap samples that cause errors\n",
    "                continue\n",
    "        \n",
    "        bootstrap_diffs = np.array(bootstrap_diffs)\n",
    "        \n",
    "        # P-value for regularized being better (lower metric)\n",
    "        p_value_reg_better = np.mean(bootstrap_diffs <= 0)\n",
    "        \n",
    "        # Confidence intervals\n",
    "        ci_95_lower = np.percentile(bootstrap_diffs, 2.5)\n",
    "        ci_95_upper = np.percentile(bootstrap_diffs, 97.5)\n",
    "        \n",
    "        return {\n",
    "            'metric_name': metric,\n",
    "            'overfit_metric': overfit_metric,\n",
    "            'regularized_metric': reg_metric,\n",
    "            'observed_difference': observed_diff,\n",
    "            'p_value_reg_better': p_value_reg_better,\n",
    "            'bootstrap_differences': bootstrap_diffs,\n",
    "            'ci_95': (ci_95_lower, ci_95_upper),\n",
    "            'regularized_is_better': observed_diff < 0\n",
    "        }\n",
    "\n",
    "    def patch_consistency_test(self, overfit_variances, reg_variances):\n",
    "        \"\"\"Test which model has more consistent patch predictions\"\"\"\n",
    "        if len(overfit_variances) > 1:\n",
    "            stat, p_value = stats.wilcoxon(reg_variances, overfit_variances, \n",
    "                                         alternative='less')  # reg < overfit\n",
    "        else:\n",
    "            stat, p_value = None, None\n",
    "        \n",
    "        return {\n",
    "            'overfit_mean_variance': np.mean(overfit_variances),\n",
    "            'regularized_mean_variance': np.mean(reg_variances),\n",
    "            'difference': np.mean(reg_variances) - np.mean(overfit_variances),\n",
    "            'wilcoxon_statistic': stat,\n",
    "            'p_value_reg_more_consistent': p_value,\n",
    "            'regularized_more_consistent': np.mean(reg_variances) < np.mean(overfit_variances)\n",
    "        }\n",
    "\n",
    "    def comprehensive_calibration_analysis(self, test_folder_path, excel_path, \n",
    "                                         sheet_name='vg_cv_data_july31'):\n",
    "        \"\"\"Run complete calibration analysis following the patch aggregation pattern\"\"\"\n",
    "        \n",
    "        print(\"COMPREHENSIVE CALIBRATION ANALYSIS - PATCH AGGREGATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. Collect patch-level data\n",
    "        print(\"\\n1. COLLECTING PATCH-LEVEL DATA...\")\n",
    "        painting_ids, true_labels, overfit_probs, reg_probs = self.collect_patch_level_data(\n",
    "            test_folder_path, excel_path, sheet_name)\n",
    "        \n",
    "        if len(painting_ids) == 0:\n",
    "            print(\"ERROR: No patch data collected!\")\n",
    "            return None\n",
    "        \n",
    "        # 2. Aggregate per painting\n",
    "        print(\"\\n2. AGGREGATING PER PAINTING WITH MAJORITY VOTING...\")\n",
    "        painting_data = self.aggregate_per_painting(painting_ids, true_labels, overfit_probs, reg_probs)\n",
    "        \n",
    "        # Extract aggregated data\n",
    "        true_labels = painting_data['true_labels']\n",
    "        overfit_preds = painting_data['overfit_predictions']\n",
    "        reg_preds = painting_data['reg_predictions']\n",
    "        overfit_confidences = painting_data['overfit_confidences']\n",
    "        reg_confidences = painting_data['reg_confidences']\n",
    "        overfit_variances = painting_data['overfit_variances']\n",
    "        reg_variances = painting_data['reg_variances']\n",
    "        \n",
    "        # Calculate correctness\n",
    "        overfit_correct = (overfit_preds == true_labels).astype(int)\n",
    "        reg_correct = (reg_preds == true_labels).astype(int)\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        overfit_acc = np.mean(overfit_correct)\n",
    "        reg_acc = np.mean(reg_correct)\n",
    "        \n",
    "        print(f\"\\n3. BASIC PERFORMANCE:\")\n",
    "        print(f\"   Number of paintings: {len(true_labels)}\")\n",
    "        print(f\"   Total patches processed: {len(painting_ids)}\")\n",
    "        print(f\"   Average patches per painting: {len(painting_ids) / len(true_labels):.1f}\")\n",
    "        print(f\"   Overfit accuracy: {overfit_acc:.4f}\")\n",
    "        print(f\"   Regularized accuracy: {reg_acc:.4f}\")\n",
    "        print(f\"   True labels: {np.sum(true_labels == 0)} authentic, {np.sum(true_labels == 1)} imitation\")\n",
    "        \n",
    "        # Print confidence distribution\n",
    "        print(f\"   Overfit confidence range: [{np.min(overfit_confidences):.3f}, {np.max(overfit_confidences):.3f}]\")\n",
    "        print(f\"   Regularized confidence range: [{np.min(reg_confidences):.3f}, {np.max(reg_confidences):.3f}]\")\n",
    "        \n",
    "        # 4. Calibration Analysis\n",
    "        print(f\"\\n4. CALIBRATION METRICS:\")\n",
    "        \n",
    "        superiority_evidence = []\n",
    "        \n",
    "        # ECE Analysis\n",
    "        ece_result = self.bootstrap_calibration_comparison(overfit_confidences, reg_confidences,\n",
    "                                                         overfit_correct, reg_correct, metric='ece')\n",
    "        print(f\"\\n   Expected Calibration Error (ECE):\")\n",
    "        print(f\"     Overfit ECE: {ece_result['overfit_metric']:.4f}\")\n",
    "        print(f\"     Regularized ECE: {ece_result['regularized_metric']:.4f}\")\n",
    "        print(f\"     Difference (reg - overfit): {ece_result['observed_difference']:.4f}\")\n",
    "        print(f\"     P(Regularized better): {ece_result['p_value_reg_better']:.4f}\")\n",
    "        print(f\"     95% CI of difference: [{ece_result['ci_95'][0]:.4f}, {ece_result['ci_95'][1]:.4f}]\")\n",
    "        \n",
    "        if ece_result['regularized_is_better'] and ece_result['p_value_reg_better'] > 0.95:\n",
    "            print(\"     Result: Regularized model is significantly better calibrated\")\n",
    "            superiority_evidence.append(\"Better calibration (ECE)\")\n",
    "        elif ece_result['regularized_is_better']:\n",
    "            print(\"     Result: Regularized model is better calibrated\")\n",
    "            superiority_evidence.append(\"Better calibration (ECE)\")\n",
    "        else:\n",
    "            print(\"     Result: Overfit model is better calibrated\")\n",
    "        \n",
    "        # Brier Score Analysis\n",
    "        brier_result = self.bootstrap_calibration_comparison(overfit_confidences, reg_confidences,\n",
    "                                                           overfit_correct, reg_correct, metric='brier')\n",
    "        print(f\"\\n   Brier Score:\")\n",
    "        print(f\"     Overfit Brier: {brier_result['overfit_metric']:.4f}\")\n",
    "        print(f\"     Regularized Brier: {brier_result['regularized_metric']:.4f}\")\n",
    "        print(f\"     Difference (reg - overfit): {brier_result['observed_difference']:.4f}\")\n",
    "        print(f\"     P(Regularized better): {brier_result['p_value_reg_better']:.4f}\")\n",
    "        print(f\"     95% CI of difference: [{brier_result['ci_95'][0]:.4f}, {brier_result['ci_95'][1]:.4f}]\")\n",
    "        \n",
    "        if brier_result['regularized_is_better'] and brier_result['p_value_reg_better'] > 0.95:\n",
    "            print(\"     Result: Regularized model has significantly better Brier score\")\n",
    "            superiority_evidence.append(\"Better Brier score\")\n",
    "        elif brier_result['regularized_is_better']:\n",
    "            print(\"     Result: Regularized model has better Brier score\")\n",
    "            superiority_evidence.append(\"Better Brier score\")\n",
    "        else:\n",
    "            print(\"     Result: Overfit model has better Brier score\")\n",
    "        \n",
    "        # Log Loss Analysis\n",
    "        logloss_result = self.bootstrap_calibration_comparison(overfit_confidences, reg_confidences,\n",
    "                                                             overfit_correct, reg_correct, metric='log_loss')\n",
    "        print(f\"\\n   Log Loss:\")\n",
    "        print(f\"     Overfit Log Loss: {logloss_result['overfit_metric']:.4f}\")\n",
    "        print(f\"     Regularized Log Loss: {logloss_result['regularized_metric']:.4f}\")\n",
    "        print(f\"     Difference (reg - overfit): {logloss_result['observed_difference']:.4f}\")\n",
    "        print(f\"     P(Regularized better): {logloss_result['p_value_reg_better']:.4f}\")\n",
    "        print(f\"     95% CI of difference: [{logloss_result['ci_95'][0]:.4f}, {logloss_result['ci_95'][1]:.4f}]\")\n",
    "        \n",
    "        if logloss_result['regularized_is_better'] and logloss_result['p_value_reg_better'] > 0.95:\n",
    "            print(\"     Result: Regularized model has significantly better log loss\")\n",
    "            superiority_evidence.append(\"Better log loss\")\n",
    "        elif logloss_result['regularized_is_better']:\n",
    "            print(\"     Result: Regularized model has better log loss\")\n",
    "            superiority_evidence.append(\"Better log loss\")\n",
    "        else:\n",
    "            print(\"     Result: Overfit model has better log loss\")\n",
    "        \n",
    "        # 5. Patch Consistency Analysis\n",
    "        print(f\"\\n5. PATCH CONSISTENCY ANALYSIS:\")\n",
    "        consistency_result = self.patch_consistency_test(overfit_variances, reg_variances)\n",
    "        print(f\"   Overfit mean patch variance: {consistency_result['overfit_mean_variance']:.4f}\")\n",
    "        print(f\"   Regularized mean patch variance: {consistency_result['regularized_mean_variance']:.4f}\")\n",
    "        print(f\"   Difference (reg - overfit): {consistency_result['difference']:.4f}\")\n",
    "        \n",
    "        if consistency_result['p_value_reg_more_consistent'] is not None:\n",
    "            print(f\"   P(Regularized more consistent): {consistency_result['p_value_reg_more_consistent']:.4f}\")\n",
    "            \n",
    "            if (consistency_result['regularized_more_consistent'] and \n",
    "                consistency_result['p_value_reg_more_consistent'] < 0.05):\n",
    "                print(\"   Result: Regularized model is significantly more consistent\")\n",
    "                superiority_evidence.append(\"Better patch consistency\")\n",
    "            elif consistency_result['regularized_more_consistent']:\n",
    "                print(\"   Result: Regularized model is more consistent\")\n",
    "                superiority_evidence.append(\"Better patch consistency\")\n",
    "            else:\n",
    "                print(\"   Result: Overfit model is more consistent\")\n",
    "        \n",
    "        # 6. Overall Assessment\n",
    "        print(f\"\\n6. OVERALL SUPERIORITY ASSESSMENT:\")\n",
    "        print(f\"   Evidence for regularized model: {len(superiority_evidence)} out of 4 metrics\")\n",
    "        for evidence in superiority_evidence:\n",
    "            print(f\"   - {evidence}\")\n",
    "        \n",
    "        if len(superiority_evidence) >= 3:\n",
    "            print(f\"\\n   CONCLUSION: Regularized model is CLEARLY SUPERIOR\")\n",
    "        elif len(superiority_evidence) >= 2:\n",
    "            print(f\"\\n   CONCLUSION: Regularized model is SUPERIOR\")\n",
    "        elif len(superiority_evidence) == 1:\n",
    "            print(f\"\\n   CONCLUSION: Regularized model is MARGINALLY BETTER\")\n",
    "        else:\n",
    "            print(f\"\\n   CONCLUSION: No clear superiority based on calibration\")\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'ece': ece_result,\n",
    "            'brier': brier_result,\n",
    "            'log_loss': logloss_result,\n",
    "            'consistency': consistency_result,\n",
    "            'superiority_evidence': superiority_evidence,\n",
    "            'is_superior': len(superiority_evidence) >= 2,\n",
    "            'sample_size': len(true_labels),\n",
    "            'accuracies': {'overfit': overfit_acc, 'regularized': reg_acc},\n",
    "            'painting_data': painting_data\n",
    "        }\n",
    "\n",
    "# Main execution function\n",
    "def run_calibration_test():\n",
    "    OVERFIT_MODEL = \"/kaggle/input/statistical-test-dataset/swin_overfit_run1.pth\"\n",
    "    REGULARIZED_MODEL = \"/kaggle/input/statistical-test-dataset/swin_regularized_run1.pth\"\n",
    "    TEST_FOLDER = \"/kaggle/input/statistical-test-dataset/test_folder/test_folder\"\n",
    "    EXCEL_PATH = \"/kaggle/input/excel-file/vg_cv_data_july31_v1_with_train_val_test_split.xlsx\"\n",
    "    \n",
    "    # Initialize calibration tester\n",
    "    tester = CalibrationTester(OVERFIT_MODEL, REGULARIZED_MODEL)\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    results = tester.comprehensive_calibration_analysis(TEST_FOLDER, EXCEL_PATH)\n",
    "    \n",
    "    return tester, results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tester, results = run_calibration_test()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8140890,
     "sourceId": 13068029,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8140946,
     "sourceId": 13068055,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 116.32262,
   "end_time": "2025-09-15T20:04:59.639140",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-15T20:03:03.316520",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
