{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c-RZlkbr_sP",
        "outputId": "ec759c48-d29e-4f49-f443-45270fa5276e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Van Gogh Inference System...\n",
            "Using device: cuda\n",
            "Aggregation method: both\n",
            "Loading swin_overfit (swin)...\n",
            "✓ swin_overfit loaded successfully\n",
            "Loading swin_regularized_1 (swin)...\n",
            "✓ swin_regularized_1 loaded successfully\n",
            "Loading swin_regularized_2 (swin)...\n",
            "✓ swin_regularized_2 loaded successfully\n",
            "Loading swin_regularized_3 (swin)...\n",
            "✓ swin_regularized_3 loaded successfully\n",
            "Loading swin_regularized_5 (swin)...\n",
            "✓ swin_regularized_5 loaded successfully\n",
            "Loading swin_regularized_6 (swin)...\n",
            "✓ swin_regularized_6 loaded successfully\n",
            "Loading swin_regularized_7 (swin)...\n",
            "✓ swin_regularized_7 loaded successfully\n",
            "Loading effnet_unstable (efficientnet)...\n",
            "✓ effnet_unstable loaded successfully\n",
            "Loading effnet_stable_1 (efficientnet)...\n",
            "✓ effnet_stable_1 loaded successfully\n",
            "Loading effnet_stable_2 (efficientnet)...\n",
            "✓ effnet_stable_2 loaded successfully\n",
            "Loading effnet_stable_3 (efficientnet)...\n",
            "✓ effnet_stable_3 loaded successfully\n",
            "Loading effnet_stable_5 (efficientnet)...\n",
            "✓ effnet_stable_5 loaded successfully\n",
            "Loading effnet_stable_6 (efficientnet)...\n",
            "✓ effnet_stable_6 loaded successfully\n",
            "Loading effnet_stable_7 (efficientnet)...\n",
            "✓ effnet_stable_7 loaded successfully\n",
            "Successfully loaded 14 models\n",
            "Found 6 images in /content/test_images\n",
            "Processing: Elimar_11MB.jp2\n",
            "Processing: Elimar_85MB.tiff\n",
            "Processing: Elimar_9MB.jpg\n",
            "Processing: Elimar_10MB.png\n",
            "Processing: Elimar_19MB.png\n",
            "Processing: Elimar_6MB.png\n",
            "\n",
            "======================================================================================================================================================\n",
            "PREDICTION RESULTS TABLE - MEAN LOGITS AGGREGATION\n",
            "======================================================================================================================================================\n",
            "                   Elimar_10MB.png Elimar_11MB.jp2 Elimar_19MB.png Elimar_6MB.png Elimar_85MB.tiff Elimar_9MB.jpg\n",
            "swin_overfit            Imit: 100%       Imit: 90%       Imit: 90%     Imit: 100%        Imit: 89%      Imit: 93%\n",
            "swin_regularized_1       Imit: 80%       Auth: 71%       Auth: 71%      Imit: 62%        Auth: 71%      Auth: 67%\n",
            "swin_regularized_2       Auth: 97%      Auth: 100%      Auth: 100%      Auth: 98%       Auth: 100%     Auth: 100%\n",
            "swin_regularized_3      Auth: 100%      Auth: 100%      Auth: 100%     Auth: 100%       Auth: 100%     Auth: 100%\n",
            "swin_regularized_5      Auth: 100%      Auth: 100%      Auth: 100%     Auth: 100%       Auth: 100%     Auth: 100%\n",
            "swin_regularized_6       Auth: 94%      Auth: 100%      Auth: 100%      Auth: 96%       Auth: 100%     Auth: 100%\n",
            "swin_regularized_7       Auth: 98%      Auth: 100%      Auth: 100%      Auth: 99%       Auth: 100%     Auth: 100%\n",
            "effnet_unstable         Imit: 100%      Imit: 100%      Imit: 100%      Imit: 96%       Imit: 100%     Imit: 100%\n",
            "effnet_stable_1          Imit: 78%       Imit: 64%       Imit: 64%      Imit: 71%        Imit: 60%      Imit: 54%\n",
            "effnet_stable_2          Auth: 66%       Auth: 99%       Auth: 99%      Auth: 73%        Auth: 99%      Auth: 98%\n",
            "effnet_stable_3          Auth: 84%       Auth: 99%       Auth: 99%      Auth: 88%        Auth: 99%      Auth: 99%\n",
            "effnet_stable_5          Auth: 96%       Auth: 99%       Auth: 99%      Auth: 97%        Auth: 99%      Auth: 99%\n",
            "effnet_stable_6          Auth: 77%       Auth: 99%       Auth: 99%      Auth: 85%        Auth: 99%      Auth: 99%\n",
            "effnet_stable_7          Auth: 92%       Auth: 99%       Auth: 99%      Auth: 95%        Auth: 99%      Auth: 99%\n",
            "======================================================================================================================================================\n",
            "\n",
            "======================================================================================================================================================\n",
            "PREDICTION RESULTS TABLE - MAJORITY VOTING AGGREGATION\n",
            "======================================================================================================================================================\n",
            "                   Elimar_10MB.png Elimar_11MB.jp2 Elimar_19MB.png Elimar_6MB.png Elimar_85MB.tiff Elimar_9MB.jpg\n",
            "swin_overfit             Imit: 98%      Imit: 100%      Imit: 100%      Imit: 90%       Imit: 100%     Imit: 100%\n",
            "swin_regularized_1       Imit: 86%       Auth: 88%       Auth: 88%      Auth: 78%        Auth: 88%      Auth: 86%\n",
            "swin_regularized_2       Auth: 94%      Auth: 100%      Auth: 100%      Auth: 92%       Auth: 100%     Auth: 100%\n",
            "swin_regularized_3       Auth: 96%      Auth: 100%      Auth: 100%      Auth: 95%       Auth: 100%     Auth: 100%\n",
            "swin_regularized_5       Auth: 96%      Auth: 100%      Auth: 100%      Auth: 96%       Auth: 100%     Auth: 100%\n",
            "swin_regularized_6       Auth: 92%      Auth: 100%      Auth: 100%      Auth: 91%       Auth: 100%     Auth: 100%\n",
            "swin_regularized_7       Auth: 95%      Auth: 100%      Auth: 100%      Auth: 98%       Auth: 100%     Auth: 100%\n",
            "effnet_unstable          Imit: 99%      Imit: 100%      Imit: 100%      Auth: 99%       Imit: 100%      Imit: 95%\n",
            "effnet_stable_1          Imit: 87%       Imit: 92%       Imit: 92%      Imit: 91%        Imit: 93%      Imit: 88%\n",
            "effnet_stable_2          Auth: 98%       Auth: 98%       Auth: 98%      Auth: 96%        Auth: 98%      Auth: 98%\n",
            "effnet_stable_3          Auth: 92%       Auth: 99%       Auth: 99%      Auth: 90%        Auth: 99%      Auth: 99%\n",
            "effnet_stable_5          Auth: 97%       Auth: 99%       Auth: 99%      Auth: 97%        Auth: 99%      Auth: 99%\n",
            "effnet_stable_6          Auth: 93%       Auth: 99%       Auth: 99%      Auth: 87%        Auth: 99%      Auth: 99%\n",
            "effnet_stable_7          Auth: 95%       Auth: 99%       Auth: 99%      Auth: 94%        Auth: 99%      Auth: 99%\n",
            "======================================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import os\n",
        "from typing import List, Dict\n",
        "\n",
        "class ConsistentVanGoghInference:\n",
        "    def __init__(self, model_configs: List[Dict], device=None, target_size=256, aggregation_method='both'):\n",
        "        self.model_configs = model_configs\n",
        "        self.target_size = target_size\n",
        "        self.aggregation_method = aggregation_method  # 'majority', 'mean', or 'both'\n",
        "\n",
        "        if device is None:\n",
        "            if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
        "                self.device = torch.device(\"mps\")\n",
        "            elif torch.cuda.is_available():\n",
        "                self.device = torch.device(\"cuda\")\n",
        "            else:\n",
        "                self.device = torch.device(\"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        print(f\"Aggregation method: {self.aggregation_method}\")\n",
        "\n",
        "        self.class_to_idx = {'authentic': 0, 'imitation': 1}\n",
        "        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}\n",
        "\n",
        "        # Load all models\n",
        "        self.models = {}\n",
        "        self._load_all_models()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((self.target_size, self.target_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                               std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        print(f\"Successfully loaded {len(self.models)} models\")\n",
        "\n",
        "    def _load_all_models(self):\n",
        "        for config in self.model_configs:\n",
        "            model_name = config['name']\n",
        "            model_type = config['type'].lower()\n",
        "            model_path = config['path']\n",
        "\n",
        "            print(f\"Loading {model_name} ({model_type})...\")\n",
        "\n",
        "            try:\n",
        "                # Create model architecture\n",
        "                if model_type == 'swin':\n",
        "                    model = timm.create_model(\n",
        "                        'swin_tiny_patch4_window7_224',\n",
        "                        pretrained=False,\n",
        "                        num_classes=2,\n",
        "                        img_size=256,\n",
        "                        drop_rate=0.5,\n",
        "                        drop_path_rate=0.4\n",
        "                    )\n",
        "                elif model_type == 'efficientnet':\n",
        "                    model = timm.create_model(\n",
        "                        'efficientnet_b5',\n",
        "                        pretrained=False,\n",
        "                        num_classes=2\n",
        "                    )\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "                # Load weights\n",
        "                state_dict = torch.load(model_path, map_location=self.device)\n",
        "                model.load_state_dict(state_dict)\n",
        "\n",
        "                model.to(self.device)\n",
        "                model.eval()\n",
        "\n",
        "                self.models[model_name] = {\n",
        "                    'model': model,\n",
        "                    'type': model_type,\n",
        "                    'category': config['category'],\n",
        "                    'path': model_path\n",
        "                }\n",
        "\n",
        "                print(f\"✓ {model_name} loaded successfully\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error loading {model_name}: {e}\")\n",
        "                raise\n",
        "\n",
        "    def _adaptive_patch_extraction(self, image):\n",
        "        w, h = image.size\n",
        "        max_dim = max(w, h)\n",
        "\n",
        "        if max_dim > 1024:\n",
        "            grid_size = 4  # 4x4 patches\n",
        "        elif max_dim >= 512:\n",
        "            grid_size = 2  # 2x2 patches\n",
        "        else:\n",
        "            grid_size = 1  # 1x1 patch\n",
        "\n",
        "        patches = []\n",
        "\n",
        "        if grid_size == 1:\n",
        "            # For small images, use center crop or resize\n",
        "            min_dim = min(w, h)\n",
        "            if min_dim < 256:\n",
        "                # Resize small images\n",
        "                resized = image.resize((256, 256), Image.Resampling.LANCZOS)\n",
        "                patches.append(resized)\n",
        "            else:\n",
        "                # Center crop for images >= 256px\n",
        "                left = (w - min_dim) // 2\n",
        "                top = (h - min_dim) // 2\n",
        "                center_crop = image.crop((left, top, left + min_dim, top + min_dim))\n",
        "                patches.append(center_crop)\n",
        "        else:\n",
        "            # Grid-based patching (non-overlapping)\n",
        "            patch_width = w // grid_size\n",
        "            patch_height = h // grid_size\n",
        "\n",
        "            for i in range(grid_size):\n",
        "                for j in range(grid_size):\n",
        "                    left = j * patch_width\n",
        "                    upper = i * patch_height\n",
        "                    # For the last patch in a row/column, extend to the image edge\n",
        "                    right = (j + 1) * patch_width if (j + 1) < grid_size else w\n",
        "                    bottom = (i + 1) * patch_height if (i + 1) < grid_size else h\n",
        "\n",
        "                    patch_img = image.crop((left, upper, right, bottom))\n",
        "                    if patch_img.size[0] > 0 and patch_img.size[1] > 0:\n",
        "                        patches.append(patch_img)\n",
        "\n",
        "        return patches\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        \"\"\"Preprocess image for model input.\"\"\"\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            return image\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict_with_adaptive_patches_single_model(self, image, model_info):\n",
        "        \"\"\"Run adaptive patch-based inference for a single model using both methods.\"\"\"\n",
        "        model = model_info['model']\n",
        "\n",
        "        patches = self._adaptive_patch_extraction(image)\n",
        "\n",
        "        if not patches:\n",
        "            raise ValueError(\"No patches extracted from image\")\n",
        "\n",
        "        # Transform patches to tensors\n",
        "        patch_tensors = []\n",
        "        for patch in patches:\n",
        "            patch_tensor = self.transform(patch).unsqueeze(0).to(self.device)\n",
        "            patch_tensors.append(patch_tensor)\n",
        "\n",
        "        # Get logits and predictions for all patches\n",
        "        all_logits = []\n",
        "        patch_predictions = []\n",
        "        patch_confidences = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for patch_tensor in patch_tensors:\n",
        "                logits = model(patch_tensor)\n",
        "                probabilities = F.softmax(logits, dim=1)\n",
        "\n",
        "                # Store logits for mean aggregation\n",
        "                all_logits.append(logits)\n",
        "\n",
        "                # Get individual patch prediction for majority voting\n",
        "                predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "                confidence = probabilities.cpu().numpy()[0]\n",
        "\n",
        "                patch_predictions.append(predicted_class_idx)\n",
        "                patch_confidences.append(confidence)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Method 1: Mean Logits Aggregation\n",
        "        if self.aggregation_method in ['mean', 'both']:\n",
        "            stacked_logits = torch.cat(all_logits, dim=0)\n",
        "            avg_logits = torch.mean(stacked_logits, dim=0, keepdim=True)\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            mean_probabilities = F.softmax(avg_logits, dim=1)\n",
        "\n",
        "            # Get prediction\n",
        "            mean_predicted_class_idx = torch.argmax(mean_probabilities, dim=1).item()\n",
        "            mean_predicted_class = self.idx_to_class[mean_predicted_class_idx]\n",
        "\n",
        "            # Get confidence scores\n",
        "            mean_confidence_scores = mean_probabilities.cpu().numpy()[0]\n",
        "            mean_max_confidence = float(mean_confidence_scores[mean_predicted_class_idx])\n",
        "\n",
        "            results['mean_logits'] = {\n",
        "                'predicted_class': mean_predicted_class,\n",
        "                'confidence': mean_max_confidence,\n",
        "                'probabilities': {\n",
        "                    'authentic': float(mean_confidence_scores[0]),\n",
        "                    'imitation': float(mean_confidence_scores[1])\n",
        "                }\n",
        "            }\n",
        "\n",
        "        # Method 2: Majority Voting\n",
        "        if self.aggregation_method in ['majority', 'both']:\n",
        "            # Count votes\n",
        "            authentic_votes = sum(1 for pred in patch_predictions if pred == 0)\n",
        "            imitation_votes = sum(1 for pred in patch_predictions if pred == 1)\n",
        "\n",
        "            # Determine final prediction based on majority\n",
        "            if authentic_votes > imitation_votes:\n",
        "                majority_final_class = 'authentic'\n",
        "            elif imitation_votes > authentic_votes:\n",
        "                majority_final_class = 'imitation'\n",
        "            else:\n",
        "                # Tie case - use average confidence to break tie\n",
        "                avg_authentic_conf = np.mean([conf[0] for conf in patch_confidences])\n",
        "                avg_imitation_conf = np.mean([conf[1] for conf in patch_confidences])\n",
        "\n",
        "                if avg_authentic_conf > avg_imitation_conf:\n",
        "                    majority_final_class = 'authentic'\n",
        "                else:\n",
        "                    majority_final_class = 'imitation'\n",
        "\n",
        "            # Calculate final confidence based on the winning class\n",
        "            if majority_final_class == 'authentic':\n",
        "                # Average confidence of patches that predicted authentic\n",
        "                authentic_confidences = [patch_confidences[i][0] for i in range(len(patch_predictions))\n",
        "                                       if patch_predictions[i] == 0]\n",
        "                if authentic_confidences:\n",
        "                    majority_final_confidence = np.mean(authentic_confidences)\n",
        "                else:\n",
        "                    majority_final_confidence = np.mean([conf[0] for conf in patch_confidences])\n",
        "            else:\n",
        "                # Average confidence of patches that predicted imitation\n",
        "                imitation_confidences = [patch_confidences[i][1] for i in range(len(patch_predictions))\n",
        "                                       if patch_predictions[i] == 1]\n",
        "                if imitation_confidences:\n",
        "                    majority_final_confidence = np.mean(imitation_confidences)\n",
        "                else:\n",
        "                    majority_final_confidence = np.mean([conf[1] for conf in patch_confidences])\n",
        "\n",
        "            # Calculate overall probabilities for reporting\n",
        "            overall_authentic_prob = authentic_votes / len(patch_predictions)\n",
        "            overall_imitation_prob = imitation_votes / len(patch_predictions)\n",
        "\n",
        "            results['majority_voting'] = {\n",
        "                'predicted_class': majority_final_class,\n",
        "                'confidence': float(majority_final_confidence),\n",
        "                'probabilities': {\n",
        "                    'authentic': float(overall_authentic_prob),\n",
        "                    'imitation': float(overall_imitation_prob)\n",
        "                },\n",
        "                'vote_counts': {\n",
        "                    'authentic': authentic_votes,\n",
        "                    'imitation': imitation_votes,\n",
        "                    'total_patches': len(patch_predictions)\n",
        "                }\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def predict_single_image(self, image_path):\n",
        "        # Load and preprocess image\n",
        "        image = self.preprocess_image(image_path)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Run inference with each model\n",
        "        for model_name, model_info in self.models.items():\n",
        "            try:\n",
        "                model_result = self.predict_with_adaptive_patches_single_model(image, model_info)\n",
        "                results[model_name] = model_result\n",
        "            except Exception as e:\n",
        "                results[model_name] = {'error': str(e)}\n",
        "\n",
        "        return results\n",
        "\n",
        "    def predict_folder_and_create_tables(self, folder_path):\n",
        "        \"\"\"Run inference on all images in a folder and create separate results tables.\"\"\"\n",
        "        folder_path = Path(folder_path)\n",
        "\n",
        "        # Supported image extensions\n",
        "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.jp2'}\n",
        "\n",
        "        # Find all image files\n",
        "        image_files = []\n",
        "        for ext in image_extensions:\n",
        "            image_files.extend(folder_path.glob(f'*{ext}'))\n",
        "            image_files.extend(folder_path.glob(f'*{ext.upper()}'))\n",
        "\n",
        "        print(f\"Found {len(image_files)} images in {folder_path}\")\n",
        "\n",
        "        if not image_files:\n",
        "            print(\"No images found!\")\n",
        "            return\n",
        "\n",
        "        # Store results for both tables\n",
        "        mean_table_data = {}\n",
        "        majority_table_data = {}\n",
        "\n",
        "        # Initialize table data structures\n",
        "        model_names = list(self.models.keys())\n",
        "        for model_name in model_names:\n",
        "            mean_table_data[model_name] = {}\n",
        "            majority_table_data[model_name] = {}\n",
        "\n",
        "        # Process each image\n",
        "        for image_path in image_files:\n",
        "            image_name = image_path.name\n",
        "            print(f\"Processing: {image_name}\")\n",
        "\n",
        "            try:\n",
        "                results = self.predict_single_image(image_path)\n",
        "\n",
        "                for model_name, result in results.items():\n",
        "                    if 'error' not in result:\n",
        "                        # Process mean logits results\n",
        "                        if 'mean_logits' in result:\n",
        "                            mean_result = result['mean_logits']\n",
        "                            pred_class = mean_result['predicted_class']\n",
        "                            confidence = mean_result['confidence']\n",
        "\n",
        "                            if pred_class == 'authentic':\n",
        "                                mean_table_data[model_name][image_name] = f\"Auth: {confidence*100:.0f}%\"\n",
        "                            else:\n",
        "                                mean_table_data[model_name][image_name] = f\"Imit: {confidence*100:.0f}%\"\n",
        "\n",
        "                        # Process majority voting results\n",
        "                        if 'majority_voting' in result:\n",
        "                            majority_result = result['majority_voting']\n",
        "                            pred_class = majority_result['predicted_class']\n",
        "                            confidence = majority_result['confidence']\n",
        "\n",
        "                            if pred_class == 'authentic':\n",
        "                                majority_table_data[model_name][image_name] = f\"Auth: {confidence*100:.0f}%\"\n",
        "                            else:\n",
        "                                majority_table_data[model_name][image_name] = f\"Imit: {confidence*100:.0f}%\"\n",
        "                    else:\n",
        "                        # Handle errors\n",
        "                        mean_table_data[model_name][image_name] = \"ERROR\"\n",
        "                        majority_table_data[model_name][image_name] = \"ERROR\"\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_name}: {e}\")\n",
        "                for model_name in model_names:\n",
        "                    mean_table_data[model_name][image_name] = \"ERROR\"\n",
        "                    majority_table_data[model_name][image_name] = \"ERROR\"\n",
        "\n",
        "        # Create and display the first table (Mean Logits)\n",
        "        mean_df = pd.DataFrame(mean_table_data).T  # Transpose to have models as rows\n",
        "        mean_df = mean_df.reindex(sorted(mean_df.columns), axis=1)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*150)\n",
        "        print(\"PREDICTION RESULTS TABLE - MEAN LOGITS AGGREGATION\")\n",
        "        print(\"=\"*150)\n",
        "        print(mean_df.to_string())\n",
        "        print(\"=\"*150)\n",
        "\n",
        "        # Create and display the second table (Majority Voting)\n",
        "        majority_df = pd.DataFrame(majority_table_data).T  # Transpose to have models as rows\n",
        "        majority_df = majority_df.reindex(sorted(majority_df.columns), axis=1)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*150)\n",
        "        print(\"PREDICTION RESULTS TABLE - MAJORITY VOTING AGGREGATION\")\n",
        "        print(\"=\"*150)\n",
        "        print(majority_df.to_string())\n",
        "        print(\"=\"*150)\n",
        "\n",
        "        return mean_df, majority_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Model configurations\n",
        "    MODEL_CONFIGS = [\n",
        "        {\n",
        "            'path': '/swin_overfit_model_run1.pth',\n",
        "            'type': 'swin',\n",
        "            'name': 'swin_overfit',\n",
        "            'category': 'overfit'\n",
        "        },\n",
        "        {\n",
        "            'path': '/swin_regularized_model_run1.pth',\n",
        "            'type': 'swin',\n",
        "            'name': 'swin_regularized_1',\n",
        "            'category': 'regularized'\n",
        "        },\n",
        "        {\n",
        "            'path': '/swin_regularized_model_run2.pth',\n",
        "            'type': 'swin',\n",
        "            'name': 'swin_regularized_2',\n",
        "            'category': 'regularized'\n",
        "        },\n",
        "        {\n",
        "            'path': '/swin_regularized_model_run3.pth',\n",
        "            'type': 'swin',\n",
        "            'name': 'swin_regularized_3',\n",
        "            'category': 'regularized'\n",
        "        },\n",
        "        {\n",
        "            'path': '/swin_regularized_model_run5.pth',\n",
        "            'type': 'swin',\n",
        "            'name': 'swin_regularized_5',\n",
        "            'category': 'regularized'\n",
        "        },\n",
        "        {\n",
        "            'path': '/swin_regularized_model_run6.pth',\n",
        "            'type': 'swin',\n",
        "            'name': 'swin_regularized_6',\n",
        "            'category': 'regularized'\n",
        "        },\n",
        "        {\n",
        "            'path': '/swin_regularized_model_run7.pth',\n",
        "            'type': 'swin',\n",
        "            'name': 'swin_regularized_7',\n",
        "            'category': 'regularized'\n",
        "        },\n",
        "        {\n",
        "            'path': '/efficientnet_unstable_model_run1.pth',\n",
        "            'type': 'efficientnet',\n",
        "            'name': 'effnet_unstable',\n",
        "            'category': 'unstable'\n",
        "        },\n",
        "        {\n",
        "            'path': '/efficientnet_stable_model_run1.pth',\n",
        "            'type': 'efficientnet',\n",
        "            'name': 'effnet_stable_1',\n",
        "            'category': 'stable'\n",
        "        },\n",
        "        {\n",
        "            'path': '/efficientnet_stable_model_run2.pth',\n",
        "            'type': 'efficientnet',\n",
        "            'name': 'effnet_stable_2',\n",
        "            'category': 'stable'\n",
        "        },\n",
        "        {\n",
        "            'path': '/efficientnet_stable_model_run3.pth',\n",
        "            'type': 'efficientnet',\n",
        "            'name': 'effnet_stable_3',\n",
        "            'category': 'stable'\n",
        "        },\n",
        "        {\n",
        "            'path': '/efficientnet_stable_model_run5.pth',\n",
        "            'type': 'efficientnet',\n",
        "            'name': 'effnet_stable_5',\n",
        "            'category': 'stable'\n",
        "        },\n",
        "        {\n",
        "            'path': '/efficientnet_stable_model_run6.pth',\n",
        "            'type': 'efficientnet',\n",
        "            'name': 'effnet_stable_6',\n",
        "            'category': 'stable'\n",
        "        },\n",
        "        {\n",
        "            'path': '/efficientnet_stable_model_run7.pth',\n",
        "            'type': 'efficientnet',\n",
        "            'name': 'effnet_stable_7',\n",
        "            'category': 'stable'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Input folder containing images\n",
        "    INPUT_FOLDER = \"/content/test_images\"\n",
        "\n",
        "    print(\"Initializing Van Gogh Inference System...\")\n",
        "\n",
        "    # Initialize inference system with both methods\n",
        "    # You can change aggregation_method to 'mean', 'majority', or 'both'\n",
        "    inferencer = ConsistentVanGoghInference(MODEL_CONFIGS, aggregation_method='both')\n",
        "\n",
        "    # Run inference and create tables\n",
        "    mean_df, majority_df = inferencer.predict_folder_and_create_tables(folder_path=INPUT_FOLDER)\n",
        "\n",
        "    return mean_df, majority_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}